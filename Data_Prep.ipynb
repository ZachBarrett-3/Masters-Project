{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11894/3925434028.py:8: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  vitals = pd.read_csv(path)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os, copy\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Read in data... just do this once.\n",
    "path = r'PATH/data/pneumona_reduced_vitals.csv'\n",
    "vitals = pd.read_csv(path)\n",
    "vitals = np.array(vitals)\n",
    "\n",
    "# Structure of vitals rows\n",
    "#ROW_ID | SUBJECT_ID | HADM_ID | ICUSTAY_ID | ITEMID | CHARTTIME | STORETIME | CGID | VALUE | VALUENUM | VALUEUOM | WARNING | ERROR | RESULTSTATUS | STOPPED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct PATIENT_IDs: 494\n",
      "Number of distinct HADM_IDs: 547\n"
     ]
    }
   ],
   "source": [
    "# Get the unique PATIENT_ID values and count them\n",
    "unique_ids = np.unique(vitals[:, 1])\n",
    "num_unique_ids = len(unique_ids)\n",
    "unique_hids = np.unique(vitals[:, 2])\n",
    "num_unique_hids = len(unique_hids)\n",
    "print(\"Number of distinct PATIENT_IDs:\", num_unique_ids)\n",
    "print(\"Number of distinct HADM_IDs:\", num_unique_hids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Pneumonia patients & write out vitals to distinct .csv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output folder if it doesn't exist\n",
    "output_folder = 'Pneumonia_sep'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Open the mixed CSV file for reading\n",
    "with open(path, 'r') as mixed_file:\n",
    "    reader = csv.DictReader(mixed_file)\n",
    "    \n",
    "    # Create a dictionary to store the output file handles\n",
    "    output_files = {}\n",
    "    \n",
    "    # Iterate over the rows in the mixed file\n",
    "    for row in reader:\n",
    "        hadm_id = row['HADM_ID']\n",
    "        output_file_name = f'{output_folder}/{hadm_id}.csv'\n",
    "        \n",
    "        # If this is the first row with this HADM ID, open a new output file\n",
    "        if hadm_id not in output_files:\n",
    "            output_files[hadm_id] = open(output_file_name, 'w', newline='')\n",
    "            writer = csv.DictWriter(output_files[hadm_id], fieldnames=reader.fieldnames)\n",
    "            writer.writeheader()\n",
    "        \n",
    "        # Write the row to the output file\n",
    "        writer = csv.DictWriter(output_files[hadm_id], fieldnames=reader.fieldnames)\n",
    "        writer.writerow(row)\n",
    "    \n",
    "    # Close all the output files\n",
    "    for output_file in output_files.values():\n",
    "        output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat each patient-visit level file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 196654.csv\n",
      "Processed 166108.csv\n",
      "Processed 107386.csv\n",
      "Processed 189705.csv\n",
      "Processed 178444.csv\n",
      "Processed 185274.csv\n",
      "Processed 141509.csv\n",
      "Processed 137482.csv\n",
      "Processed 121467.csv\n",
      "Processed 107242.csv\n",
      "Processed 129719.csv\n",
      "Processed 181155.csv\n",
      "Processed 169086.csv\n",
      "Processed 175073.csv\n",
      "Processed 167296.csv\n",
      "Processed 164079.csv\n",
      "Processed 146900.csv\n",
      "Processed 139113.csv\n",
      "Processed 158260.csv\n",
      "Processed 182547.csv\n",
      "Processed 125222.csv\n",
      "Processed 139516.csv\n",
      "Processed 181079.csv\n",
      "Processed 130819.csv\n",
      "Processed 169219.csv\n",
      "Processed 115544.csv\n",
      "Processed 139288.csv\n",
      "Processed 194210.csv\n",
      "Processed 120248.csv\n",
      "Processed 157155.csv\n",
      "Processed 138302.csv\n",
      "Processed 179553.csv\n",
      "Processed 131284.csv\n",
      "Processed 135559.csv\n",
      "Processed 171563.csv\n",
      "Processed 111350.csv\n",
      "Processed 132904.csv\n",
      "Processed 171729.csv\n",
      "Processed 103511.csv\n",
      "Processed 142369.csv\n",
      "Processed 161493.csv\n",
      "Processed 193373.csv\n",
      "Processed 170467.csv\n",
      "Processed 198584.csv\n",
      "Processed 161885.csv\n",
      "Processed 139984.csv\n",
      "Processed 181557.csv\n",
      "Processed 150908.csv\n",
      "Processed 158847.csv\n",
      "Processed 145111.csv\n",
      "Processed 119326.csv\n",
      "Processed 154321.csv\n",
      "Processed 159595.csv\n",
      "Processed 115825.csv\n",
      "Processed 191017.csv\n",
      "Processed 119185.csv\n",
      "Processed 100747.csv\n",
      "Processed 161285.csv\n",
      "Processed 132316.csv\n",
      "Processed 163500.csv\n",
      "Processed 171324.csv\n",
      "Processed 160363.csv\n",
      "Processed 158953.csv\n",
      "Processed 135705.csv\n",
      "Processed 103722.csv\n",
      "Processed 151035.csv\n",
      "Processed 121568.csv\n",
      "Processed 143269.csv\n",
      "Processed 143757.csv\n",
      "Processed 133486.csv\n",
      "Processed 188589.csv\n",
      "Processed 197200.csv\n",
      "Processed 112491.csv\n",
      "Processed 134345.csv\n",
      "Processed 168587.csv\n",
      "Processed 134466.csv\n",
      "Processed 174449.csv\n",
      "Processed 169712.csv\n",
      "Processed 153745.csv\n",
      "Processed 199052.csv\n",
      "Processed 105769.csv\n",
      "Processed 194980.csv\n",
      "Processed 198127.csv\n",
      "Processed 151363.csv\n",
      "Processed 126391.csv\n",
      "Processed 164327.csv\n",
      "Processed 122561.csv\n",
      "Processed 153781.csv\n",
      "Processed 178625.csv\n",
      "Processed 145395.csv\n",
      "Processed 137753.csv\n",
      "Processed 126651.csv\n",
      "Processed 174698.csv\n",
      "Processed 119010.csv\n",
      "Processed 159025.csv\n",
      "Processed 192317.csv\n",
      "Processed 193801.csv\n",
      "Processed 125366.csv\n",
      "Processed 174141.csv\n",
      "Processed 158069.csv\n",
      "Processed 106560.csv\n",
      "Processed 153803.csv\n",
      "Processed 165584.csv\n",
      "Processed 149416.csv\n",
      "Processed 180741.csv\n",
      "Processed 128595.csv\n",
      "Processed 100295.csv\n",
      "Processed 128110.csv\n",
      "Processed 167912.csv\n",
      "Processed 126022.csv\n",
      "Processed 138141.csv\n",
      "Processed 132914.csv\n",
      "Processed 107231.csv\n",
      "Processed 100496.csv\n",
      "Processed 152541.csv\n",
      "Processed 154392.csv\n",
      "Processed 197855.csv\n",
      "Processed 199518.csv\n",
      "Processed 134467.csv\n",
      "Processed 109675.csv\n",
      "Processed 149724.csv\n",
      "Processed 144446.csv\n",
      "Processed 195058.csv\n",
      "Processed 141603.csv\n",
      "Processed 174834.csv\n",
      "Processed 191049.csv\n",
      "Processed 141515.csv\n",
      "Processed 114340.csv\n",
      "Processed 121316.csv\n",
      "Processed 139199.csv\n",
      "Processed 168197.csv\n",
      "Processed 145973.csv\n",
      "Processed 135037.csv\n",
      "Processed 108765.csv\n",
      "Processed 162739.csv\n",
      "Processed 108797.csv\n",
      "Processed 103701.csv\n",
      "Processed 127741.csv\n",
      "Processed 190583.csv\n",
      "Processed 181588.csv\n",
      "Processed 184823.csv\n",
      "Processed 181367.csv\n",
      "Processed 126201.csv\n",
      "Processed 190132.csv\n",
      "Processed 163302.csv\n",
      "Processed 169985.csv\n",
      "Processed 118937.csv\n",
      "Processed 140165.csv\n",
      "Processed 114936.csv\n",
      "Processed 160915.csv\n",
      "Processed 176143.csv\n",
      "Processed 142434.csv\n",
      "Processed 187095.csv\n",
      "Processed 124621.csv\n",
      "Processed 181580.csv\n",
      "Processed 155250.csv\n",
      "Processed 153218.csv\n",
      "Processed 197038.csv\n",
      "Processed 147209.csv\n",
      "Processed 115115.csv\n",
      "Processed 108363.csv\n",
      "Processed 148356.csv\n",
      "Processed 172731.csv\n",
      "Processed 186894.csv\n",
      "Processed 133275.csv\n",
      "Processed 139544.csv\n",
      "Processed 117632.csv\n",
      "Processed 143444.csv\n",
      "Processed 139228.csv\n",
      "Processed 173731.csv\n",
      "Processed 112683.csv\n",
      "Processed 137091.csv\n",
      "Processed 112136.csv\n",
      "Processed 122006.csv\n",
      "Processed 194990.csv\n",
      "Processed 183659.csv\n",
      "Processed 185180.csv\n",
      "Processed 131159.csv\n",
      "Processed 124327.csv\n",
      "Processed 151927.csv\n",
      "Processed 125949.csv\n",
      "Processed 154710.csv\n",
      "Processed 149702.csv\n",
      "Processed 175392.csv\n",
      "Processed 131082.csv\n",
      "Processed 182254.csv\n",
      "Processed 163271.csv\n",
      "Processed 107175.csv\n",
      "Processed 102995.csv\n",
      "Processed 179773.csv\n",
      "Processed 125297.csv\n",
      "Processed 133150.csv\n",
      "Processed 189094.csv\n",
      "Processed 185398.csv\n",
      "Processed 169777.csv\n",
      "Processed 196887.csv\n",
      "Processed 178412.csv\n",
      "Processed 163511.csv\n",
      "Processed 184951.csv\n",
      "Processed 180602.csv\n",
      "Processed 153859.csv\n",
      "Processed 184168.csv\n",
      "Processed 108490.csv\n",
      "Processed 152869.csv\n",
      "Processed 118358.csv\n",
      "Processed 184752.csv\n",
      "Processed 110384.csv\n",
      "Processed 107710.csv\n",
      "Processed 125258.csv\n",
      "Processed 104650.csv\n",
      "Processed 104285.csv\n",
      "Processed 111369.csv\n",
      "Processed 173980.csv\n",
      "Processed 121127.csv\n",
      "Processed 197918.csv\n",
      "Processed 128247.csv\n",
      "Processed 162290.csv\n",
      "Processed 167883.csv\n",
      "Processed 178742.csv\n",
      "Processed 115683.csv\n",
      "Processed 183220.csv\n",
      "Processed 195182.csv\n",
      "Processed 191137.csv\n",
      "Processed 145622.csv\n",
      "Processed 116250.csv\n",
      "Processed 114610.csv\n",
      "Processed 172600.csv\n",
      "Processed 153922.csv\n",
      "Processed 113848.csv\n",
      "Processed 128252.csv\n",
      "Processed 113354.csv\n",
      "Processed 101701.csv\n",
      "Processed 124238.csv\n",
      "Processed 141549.csv\n",
      "Processed 175513.csv\n",
      "Processed 159562.csv\n",
      "Processed 140897.csv\n",
      "Processed 195174.csv\n",
      "Processed 148567.csv\n",
      "Processed 155751.csv\n",
      "Processed 136296.csv\n",
      "Processed 186282.csv\n",
      "Processed 154901.csv\n",
      "Processed 177542.csv\n",
      "Processed 193772.csv\n",
      "Processed 180861.csv\n",
      "Processed 135367.csv\n",
      "Processed 124928.csv\n",
      "Processed 155950.csv\n",
      "Processed 180723.csv\n",
      "Processed 179661.csv\n",
      "Processed 124313.csv\n",
      "Processed 152860.csv\n",
      "Processed 170337.csv\n",
      "Processed 135139.csv\n",
      "Processed 170408.csv\n",
      "Processed 155940.csv\n",
      "Processed 148875.csv\n",
      "Processed 166015.csv\n",
      "Processed 198463.csv\n",
      "Processed 108553.csv\n",
      "Processed 139071.csv\n",
      "Processed 160620.csv\n",
      "Processed 192925.csv\n",
      "Processed 117171.csv\n",
      "Processed 171965.csv\n",
      "Processed 172907.csv\n",
      "Processed 100985.csv\n",
      "Processed 138843.csv\n",
      "Processed 110512.csv\n",
      "Processed 109932.csv\n",
      "Processed 120956.csv\n",
      "Processed 128414.csv\n",
      "Processed 134244.csv\n",
      "Processed 115724.csv\n",
      "Processed 109242.csv\n",
      "Processed 142005.csv\n",
      "Processed 137602.csv\n",
      "Processed 185917.csv\n",
      "Processed 186416.csv\n",
      "Processed 193992.csv\n",
      "Processed 113310.csv\n",
      "Processed 113572.csv\n",
      "Processed 144175.csv\n",
      "Processed 147604.csv\n",
      "Processed 195169.csv\n",
      "Processed 156460.csv\n",
      "Processed 111487.csv\n",
      "Processed 102101.csv\n",
      "Processed 134531.csv\n",
      "Processed 131656.csv\n",
      "Processed 146641.csv\n",
      "Processed 152960.csv\n",
      "Processed 142986.csv\n",
      "Processed 185752.csv\n",
      "Processed 152032.csv\n",
      "Processed 126793.csv\n",
      "Processed 138875.csv\n",
      "Processed 126042.csv\n",
      "Processed 161593.csv\n",
      "Processed 113703.csv\n",
      "Processed 193644.csv\n",
      "Processed 104225.csv\n",
      "Processed 187428.csv\n",
      "Processed 167837.csv\n",
      "Processed 174799.csv\n",
      "Processed 159102.csv\n",
      "Processed 107524.csv\n",
      "Processed 195471.csv\n",
      "Processed 121321.csv\n",
      "Processed 151973.csv\n",
      "Processed 188445.csv\n",
      "Processed 118513.csv\n",
      "Processed 132899.csv\n",
      "Processed 147245.csv\n",
      "Processed 199391.csv\n",
      "Processed 156598.csv\n",
      "Processed 148775.csv\n",
      "Processed 174292.csv\n",
      "Processed 171762.csv\n",
      "Processed 150375.csv\n",
      "Processed 138950.csv\n",
      "Processed 185754.csv\n",
      "Processed 116562.csv\n",
      "Processed 132669.csv\n",
      "Processed 125028.csv\n",
      "Processed 160011.csv\n",
      "Processed 170675.csv\n",
      "Processed 110193.csv\n",
      "Processed 114246.csv\n",
      "Processed 197057.csv\n",
      "Processed 193392.csv\n",
      "Processed 151031.csv\n",
      "Processed 163043.csv\n",
      "Processed 171431.csv\n",
      "Processed 124933.csv\n",
      "Processed 108611.csv\n",
      "Processed 177598.csv\n",
      "Processed 178923.csv\n",
      "Processed 198608.csv\n",
      "Processed 182075.csv\n",
      "Processed 177694.csv\n",
      "Processed 122781.csv\n",
      "Processed 101665.csv\n",
      "Processed 176772.csv\n",
      "Processed 174252.csv\n",
      "Processed 175079.csv\n",
      "Processed 181920.csv\n",
      "Processed 108859.csv\n",
      "Processed 159574.csv\n",
      "Processed 110529.csv\n",
      "Processed 155477.csv\n",
      "Processed 197892.csv\n",
      "Processed 163710.csv\n",
      "Processed 186599.csv\n",
      "Processed 165614.csv\n",
      "Processed 199175.csv\n",
      "Processed 116133.csv\n",
      "Processed 199951.csv\n",
      "Processed 154628.csv\n",
      "Processed 140477.csv\n",
      "Processed 134463.csv\n",
      "Processed 150479.csv\n",
      "Processed 177601.csv\n",
      "Processed 143606.csv\n",
      "Processed 161581.csv\n",
      "Processed 199510.csv\n",
      "Processed 152495.csv\n",
      "Processed 101787.csv\n",
      "Processed 126815.csv\n",
      "Processed 173410.csv\n",
      "Processed 126172.csv\n",
      "Processed 100325.csv\n",
      "Processed 162973.csv\n",
      "Processed 192475.csv\n",
      "Processed 172082.csv\n",
      "Processed 176997.csv\n",
      "Processed 194502.csv\n",
      "Processed 134668.csv\n",
      "Processed 141505.csv\n",
      "Processed 177035.csv\n",
      "Processed 138817.csv\n",
      "Processed 148770.csv\n",
      "Processed 116190.csv\n",
      "Processed 181675.csv\n",
      "Processed 112902.csv\n",
      "Processed 127545.csv\n",
      "Processed 186618.csv\n",
      "Processed 113099.csv\n",
      "Processed 194828.csv\n",
      "Processed 152893.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 107604.csv\n",
      "Processed 100016.csv\n",
      "Processed 183957.csv\n",
      "Processed 140317.csv\n",
      "Processed 194269.csv\n",
      "Processed 125391.csv\n",
      "Processed 187968.csv\n",
      "Processed 157970.csv\n",
      "Processed 185964.csv\n",
      "Processed 106393.csv\n",
      "Processed 104794.csv\n",
      "Processed 189921.csv\n",
      "Processed 186382.csv\n",
      "Processed 165706.csv\n",
      "Processed 171657.csv\n",
      "Processed 186024.csv\n",
      "Processed 108863.csv\n",
      "Processed 197435.csv\n",
      "Processed 113326.csv\n",
      "Processed 188519.csv\n",
      "Processed 178290.csv\n",
      "Processed 182095.csv\n",
      "Processed 157169.csv\n",
      "Processed 101272.csv\n",
      "Processed 128804.csv\n",
      "Processed 188443.csv\n",
      "Processed 186080.csv\n",
      "Processed 188981.csv\n",
      "Processed 100781.csv\n",
      "Processed 130946.csv\n",
      "Processed 152968.csv\n",
      "Processed 163369.csv\n",
      "Processed 143930.csv\n",
      "Processed 158021.csv\n",
      "Processed 140887.csv\n",
      "Processed 138319.csv\n",
      "Processed 120051.csv\n",
      "Processed 151964.csv\n",
      "Processed 120654.csv\n",
      "Processed 123865.csv\n",
      "Processed 171144.csv\n",
      "Processed 146849.csv\n",
      "Processed 129414.csv\n",
      "Processed 166437.csv\n",
      "Processed 144362.csv\n",
      "Processed 155834.csv\n",
      "Processed 160367.csv\n",
      "Processed 132342.csv\n",
      "Processed 125206.csv\n",
      "Processed 107550.csv\n",
      "Processed 126060.csv\n",
      "Processed 135812.csv\n",
      "Processed 142840.csv\n",
      "Processed 184819.csv\n",
      "Processed 171254.csv\n",
      "Processed 167934.csv\n",
      "Processed 133421.csv\n",
      "Processed 160196.csv\n",
      "Processed 170602.csv\n",
      "Processed 124655.csv\n",
      "Processed 120602.csv\n",
      "Processed 169629.csv\n",
      "Processed 115106.csv\n",
      "Processed 188265.csv\n",
      "Processed 122524.csv\n",
      "Processed 189387.csv\n",
      "Processed 160098.csv\n",
      "Processed 156035.csv\n",
      "Processed 117648.csv\n",
      "Processed 156837.csv\n",
      "Processed 126103.csv\n",
      "Processed 178213.csv\n",
      "Processed 130639.csv\n",
      "Processed 149107.csv\n",
      "Processed 174568.csv\n",
      "Processed 117914.csv\n",
      "Processed 197390.csv\n",
      "Processed 130556.csv\n",
      "Processed 147659.csv\n",
      "Processed 102286.csv\n",
      "Processed 157104.csv\n",
      "Processed 134182.csv\n",
      "Processed 172620.csv\n",
      "Processed 135003.csv\n",
      "Processed 182233.csv\n",
      "Processed 101651.csv\n",
      "Processed 145392.csv\n",
      "Processed 146313.csv\n",
      "Processed 175544.csv\n",
      "Processed 197330.csv\n",
      "Processed 156145.csv\n",
      "Processed 108829.csv\n",
      "Processed 117330.csv\n",
      "Processed 191858.csv\n",
      "Processed 153334.csv\n",
      "Processed 142525.csv\n",
      "Processed 132178.csv\n",
      "Processed 122435.csv\n",
      "Processed 129375.csv\n",
      "Processed 100890.csv\n",
      "Processed 107252.csv\n",
      "Processed 104504.csv\n",
      "Processed 144190.csv\n",
      "Processed 179589.csv\n",
      "Processed 199597.csv\n",
      "Processed 184489.csv\n",
      "Processed 123407.csv\n",
      "Processed 117516.csv\n",
      "Processed 108833.csv\n",
      "Processed 113635.csv\n",
      "Processed 143418.csv\n",
      "Processed 143069.csv\n",
      "Processed 132083.csv\n",
      "Processed 183250.csv\n",
      "Processed 100524.csv\n",
      "Processed 131337.csv\n",
      "Processed 180395.csv\n",
      "Processed 150026.csv\n",
      "Processed 162146.csv\n",
      "Processed 132874.csv\n",
      "Processed 161137.csv\n",
      "Processed 132593.csv\n",
      "Processed 188494.csv\n",
      "Processed 128711.csv\n",
      "Processed 122948.csv\n",
      "Processed 169535.csv\n",
      "Processed 102510.csv\n",
      "Processed 125449.csv\n",
      "Processed 119935.csv\n",
      "Processed 168552.csv\n",
      "Processed 100030.csv\n",
      "Processed 177861.csv\n",
      "Processed 135955.csv\n",
      "Processed 173350.csv\n",
      "Processed 112404.csv\n",
      "Processed 123405.csv\n",
      "Processed 160522.csv\n",
      "Processed 139837.csv\n",
      "Processed 183106.csv\n",
      "Processed 192055.csv\n",
      "Processed 167446.csv\n",
      "Processed 158751.csv\n",
      "Processed 184923.csv\n",
      "Processed 190816.csv\n",
      "Processed 172546.csv\n",
      "Processed 135910.csv\n",
      "Processed 145181.csv\n",
      "Processed 127542.csv\n",
      "Processed 103539.csv\n",
      "Processed 178324.csv\n",
      "Processed 165916.csv\n",
      "Processed 155784.csv\n",
      "Processed 179037.csv\n",
      "Processed 102195.csv\n",
      "Processed 119132.csv\n",
      "Processed 180416.csv\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Set up the file paths for the input and output directories\n",
    "input_dir = r'PATH/data/Pneumonia_sep'\n",
    "output_dir = r'PATH/data/Pneumonia_sep_processed'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# create a list of the ITEMID values you want to extract\n",
    "item_ids = [211, 220045, #Heart Rate\n",
    "            646, 220227, #Sp02 (blood oxygen level)\n",
    "            8368, 220051, #Arterial BP Diastolic\n",
    "            51, 220050, #Arterial BP Systolic\n",
    "            615, 224690] #Total Resp\n",
    "\n",
    "csv_files = [filename for filename in os.listdir(input_dir) if filename.endswith('.csv')]\n",
    "\n",
    "# loop over each file in the input directory\n",
    "for filename in csv_files:\n",
    "    # read the CSV file into a pandas dataframe\n",
    "    df = pd.read_csv(os.path.join(input_dir, filename))\n",
    "    \n",
    "    # group the dataframe by CHARTTIME\n",
    "    grouped = df.groupby(['CHARTTIME'])\n",
    "    result = pd.DataFrame()\n",
    "\n",
    "    # iterate over each group and extract the ITEMID values\n",
    "    for name, group in grouped:\n",
    "        # create a new row for the result dataframe\n",
    "        row = {'CHARTTIME': name}\n",
    "        for item_id in item_ids:\n",
    "            # extract the rows with the specified ITEMID at CHARTTIME\n",
    "            filtered = group[(group['ITEMID'] == item_id) & (group['CHARTTIME'] == name)]\n",
    "            if not filtered.empty:\n",
    "                # if there is a row with the ITEMID at CHARTTIME, extract the value\n",
    "                row[f'ITEMID_{item_id}'] = filtered.iloc[0]['VALUE']\n",
    "            else:\n",
    "                # if there is no row with the ITEMID at CHARTTIME, leave the spot blank\n",
    "                row[f'ITEMID_{item_id}'] = ''\n",
    "        # add the row to the result dataframe\n",
    "        result = result.append(row, ignore_index=True)\n",
    "\n",
    "    # save the result dataframe to a new CSV file in the output directory\n",
    "    output_filename = os.path.join(output_dir, filename)\n",
    "    result.to_csv(output_filename, index=False)\n",
    "    print(f\"Processed {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 547 CSV files in Pneumonia_sep_processed.\n"
     ]
    }
   ],
   "source": [
    "folder_path = r'PATH/data/Pneumonia_sep_processed'\n",
    "csv_files = [filename for filename in os.listdir(folder_path) if filename.endswith('.csv')]\n",
    "num_csv_files = len(csv_files)\n",
    "print(f\"There are {num_csv_files} CSV files in Pneumonia_sep_processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the columns of equivalent readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 196654.csv\n",
      "Processed 166108.csv\n",
      "Processed 107386.csv\n",
      "Processed 189705.csv\n",
      "Processed 178444.csv\n",
      "Processed 185274.csv\n",
      "Processed 141509.csv\n",
      "Processed 137482.csv\n",
      "Processed 121467.csv\n",
      "Processed 107242.csv\n",
      "Processed 129719.csv\n",
      "Processed 181155.csv\n",
      "Processed 169086.csv\n",
      "Processed 175073.csv\n",
      "Processed 167296.csv\n",
      "Processed 164079.csv\n",
      "Processed 146900.csv\n",
      "Processed 139113.csv\n",
      "Processed 158260.csv\n",
      "Processed 182547.csv\n",
      "Processed 125222.csv\n",
      "Processed 139516.csv\n",
      "Processed 181079.csv\n",
      "Processed 130819.csv\n",
      "Processed 169219.csv\n",
      "Processed 115544.csv\n",
      "Processed 139288.csv\n",
      "Processed 194210.csv\n",
      "Processed 120248.csv\n",
      "Processed 157155.csv\n",
      "Processed 138302.csv\n",
      "Processed 179553.csv\n",
      "Processed 131284.csv\n",
      "Processed 135559.csv\n",
      "Processed 171563.csv\n",
      "Processed 111350.csv\n",
      "Processed 132904.csv\n",
      "Processed 171729.csv\n",
      "Processed 103511.csv\n",
      "Processed 142369.csv\n",
      "Processed 161493.csv\n",
      "Processed 193373.csv\n",
      "Processed 170467.csv\n",
      "Processed 198584.csv\n",
      "Processed 161885.csv\n",
      "Processed 139984.csv\n",
      "Processed 181557.csv\n",
      "Processed 150908.csv\n",
      "Processed 158847.csv\n",
      "Processed 145111.csv\n",
      "Processed 119326.csv\n",
      "Processed 154321.csv\n",
      "Processed 159595.csv\n",
      "Processed 115825.csv\n",
      "Processed 191017.csv\n",
      "Processed 119185.csv\n",
      "Processed 100747.csv\n",
      "Processed 161285.csv\n",
      "Processed 132316.csv\n",
      "Processed 163500.csv\n",
      "Processed 171324.csv\n",
      "Processed 160363.csv\n",
      "Processed 158953.csv\n",
      "Processed 135705.csv\n",
      "Processed 103722.csv\n",
      "Processed 151035.csv\n",
      "Processed 121568.csv\n",
      "Processed 143269.csv\n",
      "Processed 143757.csv\n",
      "Processed 133486.csv\n",
      "Processed 188589.csv\n",
      "Processed 197200.csv\n",
      "Processed 112491.csv\n",
      "Processed 134345.csv\n",
      "Processed 168587.csv\n",
      "Processed 134466.csv\n",
      "Processed 174449.csv\n",
      "Processed 169712.csv\n",
      "Processed 153745.csv\n",
      "Processed 199052.csv\n",
      "Processed 105769.csv\n",
      "Processed 194980.csv\n",
      "Processed 198127.csv\n",
      "Processed 151363.csv\n",
      "Processed 126391.csv\n",
      "Processed 164327.csv\n",
      "Processed 122561.csv\n",
      "Processed 153781.csv\n",
      "Processed 178625.csv\n",
      "Processed 145395.csv\n",
      "Processed 137753.csv\n",
      "Processed 126651.csv\n",
      "Processed 174698.csv\n",
      "Processed 119010.csv\n",
      "Processed 159025.csv\n",
      "Processed 192317.csv\n",
      "Processed 193801.csv\n",
      "Processed 125366.csv\n",
      "Processed 174141.csv\n",
      "Processed 158069.csv\n",
      "Processed 106560.csv\n",
      "Processed 153803.csv\n",
      "Processed 165584.csv\n",
      "Processed 149416.csv\n",
      "Processed 180741.csv\n",
      "Processed 128595.csv\n",
      "Processed 100295.csv\n",
      "Processed 128110.csv\n",
      "Processed 167912.csv\n",
      "Processed 126022.csv\n",
      "Processed 138141.csv\n",
      "Processed 132914.csv\n",
      "Processed 107231.csv\n",
      "Processed 100496.csv\n",
      "Processed 152541.csv\n",
      "Processed 154392.csv\n",
      "Processed 197855.csv\n",
      "Processed 199518.csv\n",
      "Processed 134467.csv\n",
      "Processed 109675.csv\n",
      "Processed 149724.csv\n",
      "Processed 144446.csv\n",
      "Processed 195058.csv\n",
      "Processed 141603.csv\n",
      "Processed 174834.csv\n",
      "Processed 191049.csv\n",
      "Processed 141515.csv\n",
      "Processed 114340.csv\n",
      "Processed 121316.csv\n",
      "Processed 139199.csv\n",
      "Processed 168197.csv\n",
      "Processed 145973.csv\n",
      "Processed 135037.csv\n",
      "Processed 108765.csv\n",
      "Processed 162739.csv\n",
      "Processed 108797.csv\n",
      "Processed 103701.csv\n",
      "Processed 127741.csv\n",
      "Processed 190583.csv\n",
      "Processed 181588.csv\n",
      "Processed 184823.csv\n",
      "Processed 181367.csv\n",
      "Processed 126201.csv\n",
      "Processed 190132.csv\n",
      "Processed 163302.csv\n",
      "Processed 169985.csv\n",
      "Processed 118937.csv\n",
      "Processed 140165.csv\n",
      "Processed 114936.csv\n",
      "Processed 160915.csv\n",
      "Processed 176143.csv\n",
      "Processed 142434.csv\n",
      "Processed 187095.csv\n",
      "Processed 124621.csv\n",
      "Processed 181580.csv\n",
      "Processed 155250.csv\n",
      "Processed 153218.csv\n",
      "Processed 197038.csv\n",
      "Processed 147209.csv\n",
      "Processed 115115.csv\n",
      "Processed 108363.csv\n",
      "Processed 148356.csv\n",
      "Processed 172731.csv\n",
      "Processed 186894.csv\n",
      "Processed 133275.csv\n",
      "Processed 139544.csv\n",
      "Processed 117632.csv\n",
      "Processed 143444.csv\n",
      "Processed 139228.csv\n",
      "Processed 173731.csv\n",
      "Processed 112683.csv\n",
      "Processed 137091.csv\n",
      "Processed 112136.csv\n",
      "Processed 122006.csv\n",
      "Processed 194990.csv\n",
      "Processed 183659.csv\n",
      "Processed 185180.csv\n",
      "Processed 131159.csv\n",
      "Processed 124327.csv\n",
      "Processed 151927.csv\n",
      "Processed 125949.csv\n",
      "Processed 154710.csv\n",
      "Processed 149702.csv\n",
      "Processed 175392.csv\n",
      "Processed 131082.csv\n",
      "Processed 182254.csv\n",
      "Processed 163271.csv\n",
      "Processed 107175.csv\n",
      "Processed 102995.csv\n",
      "Processed 179773.csv\n",
      "Processed 125297.csv\n",
      "Processed 133150.csv\n",
      "Processed 189094.csv\n",
      "Processed 185398.csv\n",
      "Processed 169777.csv\n",
      "Processed 196887.csv\n",
      "Processed 178412.csv\n",
      "Processed 163511.csv\n",
      "Processed 184951.csv\n",
      "Processed 180602.csv\n",
      "Processed 153859.csv\n",
      "Processed 184168.csv\n",
      "Processed 108490.csv\n",
      "Processed 152869.csv\n",
      "Processed 118358.csv\n",
      "Processed 184752.csv\n",
      "Processed 110384.csv\n",
      "Processed 107710.csv\n",
      "Processed 125258.csv\n",
      "Processed 104650.csv\n",
      "Processed 104285.csv\n",
      "Processed 111369.csv\n",
      "Processed 173980.csv\n",
      "Processed 121127.csv\n",
      "Processed 197918.csv\n",
      "Processed 128247.csv\n",
      "Processed 162290.csv\n",
      "Processed 167883.csv\n",
      "Processed 178742.csv\n",
      "Processed 115683.csv\n",
      "Processed 183220.csv\n",
      "Processed 195182.csv\n",
      "Processed 191137.csv\n",
      "Processed 145622.csv\n",
      "Processed 116250.csv\n",
      "Processed 114610.csv\n",
      "Processed 172600.csv\n",
      "Processed 153922.csv\n",
      "Processed 113848.csv\n",
      "Processed 128252.csv\n",
      "Processed 113354.csv\n",
      "Processed 101701.csv\n",
      "Processed 124238.csv\n",
      "Processed 141549.csv\n",
      "Processed 175513.csv\n",
      "Processed 159562.csv\n",
      "Processed 140897.csv\n",
      "Processed 195174.csv\n",
      "Processed 148567.csv\n",
      "Processed 155751.csv\n",
      "Processed 136296.csv\n",
      "Processed 186282.csv\n",
      "Processed 154901.csv\n",
      "Processed 177542.csv\n",
      "Processed 193772.csv\n",
      "Processed 180861.csv\n",
      "Processed 135367.csv\n",
      "Processed 124928.csv\n",
      "Processed 155950.csv\n",
      "Processed 180723.csv\n",
      "Processed 179661.csv\n",
      "Processed 124313.csv\n",
      "Processed 152860.csv\n",
      "Processed 170337.csv\n",
      "Processed 135139.csv\n",
      "Processed 170408.csv\n",
      "Processed 155940.csv\n",
      "Processed 148875.csv\n",
      "Processed 166015.csv\n",
      "Processed 198463.csv\n",
      "Processed 108553.csv\n",
      "Processed 139071.csv\n",
      "Processed 160620.csv\n",
      "Processed 192925.csv\n",
      "Processed 117171.csv\n",
      "Processed 171965.csv\n",
      "Processed 172907.csv\n",
      "Processed 100985.csv\n",
      "Processed 138843.csv\n",
      "Processed 110512.csv\n",
      "Processed 109932.csv\n",
      "Processed 120956.csv\n",
      "Processed 128414.csv\n",
      "Processed 134244.csv\n",
      "Processed 115724.csv\n",
      "Processed 109242.csv\n",
      "Processed 142005.csv\n",
      "Processed 137602.csv\n",
      "Processed 185917.csv\n",
      "Processed 186416.csv\n",
      "Processed 193992.csv\n",
      "Processed 113310.csv\n",
      "Processed 113572.csv\n",
      "Processed 144175.csv\n",
      "Processed 147604.csv\n",
      "Processed 195169.csv\n",
      "Processed 156460.csv\n",
      "Processed 111487.csv\n",
      "Processed 102101.csv\n",
      "Processed 134531.csv\n",
      "Processed 131656.csv\n",
      "Processed 146641.csv\n",
      "Processed 152960.csv\n",
      "Processed 142986.csv\n",
      "Processed 185752.csv\n",
      "Processed 152032.csv\n",
      "Processed 126793.csv\n",
      "Processed 138875.csv\n",
      "Processed 126042.csv\n",
      "Processed 161593.csv\n",
      "Processed 113703.csv\n",
      "Processed 193644.csv\n",
      "Processed 104225.csv\n",
      "Processed 187428.csv\n",
      "Processed 167837.csv\n",
      "Processed 174799.csv\n",
      "Processed 159102.csv\n",
      "Processed 107524.csv\n",
      "Processed 195471.csv\n",
      "Processed 121321.csv\n",
      "Processed 151973.csv\n",
      "Processed 188445.csv\n",
      "Processed 118513.csv\n",
      "Processed 132899.csv\n",
      "Processed 147245.csv\n",
      "Processed 199391.csv\n",
      "Processed 156598.csv\n",
      "Processed 148775.csv\n",
      "Processed 174292.csv\n",
      "Processed 171762.csv\n",
      "Processed 150375.csv\n",
      "Processed 138950.csv\n",
      "Processed 185754.csv\n",
      "Processed 116562.csv\n",
      "Processed 132669.csv\n",
      "Processed 125028.csv\n",
      "Processed 160011.csv\n",
      "Processed 170675.csv\n",
      "Processed 110193.csv\n",
      "Processed 114246.csv\n",
      "Processed 197057.csv\n",
      "Processed 193392.csv\n",
      "Processed 151031.csv\n",
      "Processed 163043.csv\n",
      "Processed 171431.csv\n",
      "Processed 124933.csv\n",
      "Processed 108611.csv\n",
      "Processed 177598.csv\n",
      "Processed 178923.csv\n",
      "Processed 198608.csv\n",
      "Processed 182075.csv\n",
      "Processed 177694.csv\n",
      "Processed 122781.csv\n",
      "Processed 101665.csv\n",
      "Processed 176772.csv\n",
      "Processed 174252.csv\n",
      "Processed 175079.csv\n",
      "Processed 181920.csv\n",
      "Processed 108859.csv\n",
      "Processed 159574.csv\n",
      "Processed 110529.csv\n",
      "Processed 155477.csv\n",
      "Processed 197892.csv\n",
      "Processed 163710.csv\n",
      "Processed 186599.csv\n",
      "Processed 165614.csv\n",
      "Processed 199175.csv\n",
      "Processed 116133.csv\n",
      "Processed 199951.csv\n",
      "Processed 154628.csv\n",
      "Processed 140477.csv\n",
      "Processed 134463.csv\n",
      "Processed 150479.csv\n",
      "Processed 177601.csv\n",
      "Processed 143606.csv\n",
      "Processed 161581.csv\n",
      "Processed 199510.csv\n",
      "Processed 152495.csv\n",
      "Processed 101787.csv\n",
      "Processed 126815.csv\n",
      "Processed 173410.csv\n",
      "Processed 126172.csv\n",
      "Processed 100325.csv\n",
      "Processed 162973.csv\n",
      "Processed 192475.csv\n",
      "Processed 172082.csv\n",
      "Processed 176997.csv\n",
      "Processed 194502.csv\n",
      "Processed 134668.csv\n",
      "Processed 141505.csv\n",
      "Processed 177035.csv\n",
      "Processed 138817.csv\n",
      "Processed 148770.csv\n",
      "Processed 116190.csv\n",
      "Processed 181675.csv\n",
      "Processed 112902.csv\n",
      "Processed 127545.csv\n",
      "Processed 186618.csv\n",
      "Processed 113099.csv\n",
      "Processed 194828.csv\n",
      "Processed 152893.csv\n",
      "Processed 107604.csv\n",
      "Processed 100016.csv\n",
      "Processed 183957.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 140317.csv\n",
      "Processed 194269.csv\n",
      "Processed 125391.csv\n",
      "Processed 187968.csv\n",
      "Processed 157970.csv\n",
      "Processed 185964.csv\n",
      "Processed 106393.csv\n",
      "Processed 104794.csv\n",
      "Processed 189921.csv\n",
      "Processed 186382.csv\n",
      "Processed 165706.csv\n",
      "Processed 171657.csv\n",
      "Processed 186024.csv\n",
      "Processed 108863.csv\n",
      "Processed 197435.csv\n",
      "Processed 113326.csv\n",
      "Processed 188519.csv\n",
      "Processed 178290.csv\n",
      "Processed 182095.csv\n",
      "Processed 157169.csv\n",
      "Processed 101272.csv\n",
      "Processed 128804.csv\n",
      "Processed 188443.csv\n",
      "Processed 186080.csv\n",
      "Processed 188981.csv\n",
      "Processed 100781.csv\n",
      "Processed 130946.csv\n",
      "Processed 152968.csv\n",
      "Processed 163369.csv\n",
      "Processed 143930.csv\n",
      "Processed 158021.csv\n",
      "Processed 140887.csv\n",
      "Processed 138319.csv\n",
      "Processed 120051.csv\n",
      "Processed 151964.csv\n",
      "Processed 120654.csv\n",
      "Processed 123865.csv\n",
      "Processed 171144.csv\n",
      "Processed 146849.csv\n",
      "Processed 129414.csv\n",
      "Processed 166437.csv\n",
      "Processed 144362.csv\n",
      "Processed 155834.csv\n",
      "Processed 160367.csv\n",
      "Processed 132342.csv\n",
      "Processed 125206.csv\n",
      "Processed 107550.csv\n",
      "Processed 126060.csv\n",
      "Processed 135812.csv\n",
      "Processed 142840.csv\n",
      "Processed 184819.csv\n",
      "Processed 171254.csv\n",
      "Processed 167934.csv\n",
      "Processed 133421.csv\n",
      "Processed 160196.csv\n",
      "Processed 170602.csv\n",
      "Processed 124655.csv\n",
      "Processed 120602.csv\n",
      "Processed 169629.csv\n",
      "Processed 115106.csv\n",
      "Processed 188265.csv\n",
      "Processed 122524.csv\n",
      "Processed 189387.csv\n",
      "Processed 160098.csv\n",
      "Processed 156035.csv\n",
      "Processed 117648.csv\n",
      "Processed 156837.csv\n",
      "Processed 126103.csv\n",
      "Processed 178213.csv\n",
      "Processed 130639.csv\n",
      "Processed 149107.csv\n",
      "Processed 174568.csv\n",
      "Processed 117914.csv\n",
      "Processed 197390.csv\n",
      "Processed 130556.csv\n",
      "Processed 147659.csv\n",
      "Processed 102286.csv\n",
      "Processed 157104.csv\n",
      "Processed 134182.csv\n",
      "Processed 172620.csv\n",
      "Processed 135003.csv\n",
      "Processed 182233.csv\n",
      "Processed 101651.csv\n",
      "Processed 145392.csv\n",
      "Processed 146313.csv\n",
      "Processed 175544.csv\n",
      "Processed 197330.csv\n",
      "Processed 156145.csv\n",
      "Processed 108829.csv\n",
      "Processed 117330.csv\n",
      "Processed 191858.csv\n",
      "Processed 153334.csv\n",
      "Processed 142525.csv\n",
      "Processed 132178.csv\n",
      "Processed 122435.csv\n",
      "Processed 129375.csv\n",
      "Processed 100890.csv\n",
      "Processed 107252.csv\n",
      "Processed 104504.csv\n",
      "Processed 144190.csv\n",
      "Processed 179589.csv\n",
      "Processed 199597.csv\n",
      "Processed 184489.csv\n",
      "Processed 123407.csv\n",
      "Processed 117516.csv\n",
      "Processed 108833.csv\n",
      "Processed 113635.csv\n",
      "Processed 143418.csv\n",
      "Processed 143069.csv\n",
      "Processed 132083.csv\n",
      "Processed 183250.csv\n",
      "Processed 100524.csv\n",
      "Processed 131337.csv\n",
      "Processed 180395.csv\n",
      "Processed 150026.csv\n",
      "Processed 162146.csv\n",
      "Processed 132874.csv\n",
      "Processed 161137.csv\n",
      "Processed 132593.csv\n",
      "Processed 188494.csv\n",
      "Processed 128711.csv\n",
      "Processed 122948.csv\n",
      "Processed 169535.csv\n",
      "Processed 102510.csv\n",
      "Processed 125449.csv\n",
      "Processed 119935.csv\n",
      "Processed 168552.csv\n",
      "Processed 100030.csv\n",
      "Processed 177861.csv\n",
      "Processed 135955.csv\n",
      "Processed 173350.csv\n",
      "Processed 112404.csv\n",
      "Processed 123405.csv\n",
      "Processed 160522.csv\n",
      "Processed 139837.csv\n",
      "Processed 183106.csv\n",
      "Processed 192055.csv\n",
      "Processed 167446.csv\n",
      "Processed 158751.csv\n",
      "Processed 184923.csv\n",
      "Processed 190816.csv\n",
      "Processed 172546.csv\n",
      "Processed 135910.csv\n",
      "Processed 145181.csv\n",
      "Processed 127542.csv\n",
      "Processed 103539.csv\n",
      "Processed 178324.csv\n",
      "Processed 165916.csv\n",
      "Processed 155784.csv\n",
      "Processed 179037.csv\n",
      "Processed 102195.csv\n",
      "Processed 119132.csv\n",
      "Processed 180416.csv\n"
     ]
    }
   ],
   "source": [
    "# Set up the file paths for the input and output directories\n",
    "input_dir = r'PATH/data/Pneumonia_sep_processed'\n",
    "output_dir = r'PATH/data/Pneumonia_sep_col_combined'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "# loop through each csv file in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        # read in the csv file\n",
    "        df = pd.read_csv(os.path.join(input_dir, filename))\n",
    "        \n",
    "        # combine the specified columns and rename them\n",
    "        df['HR'] = df[['ITEMID_211', 'ITEMID_220045']].fillna(method='backfill', axis=1)['ITEMID_211']\n",
    "        df['spo2'] = df[['ITEMID_646', 'ITEMID_220227']].fillna(method='backfill', axis=1)['ITEMID_646']\n",
    "        df['dias'] = df[['ITEMID_8368', 'ITEMID_220051']].fillna(method='backfill', axis=1)['ITEMID_8368']\n",
    "        df['sys'] = df[['ITEMID_51', 'ITEMID_220050']].fillna(method='backfill', axis=1)['ITEMID_51']\n",
    "        df['resp'] = df[['ITEMID_615', 'ITEMID_224690']].fillna(method='backfill', axis=1)['ITEMID_615']\n",
    "        \n",
    "        # drop the original columns & reorder what's left\n",
    "        df = df.drop(['ITEMID_211', 'ITEMID_220045', 'ITEMID_646', 'ITEMID_220227', \n",
    "                      'ITEMID_8368', 'ITEMID_220051', 'ITEMID_51', 'ITEMID_220050', 'ITEMID_615', 'ITEMID_224690'], axis=1)\n",
    "        df = df[['CHARTTIME', 'HR', 'sys', 'dias', #'mean',\n",
    "                 'resp','spo2']] \n",
    "\n",
    "        # write the modified dataframe to a new csv file in the output directory\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Processed {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 547 CSV files in Pneumonia_sep_col_combined.\n"
     ]
    }
   ],
   "source": [
    "folder_path = r'PATH/data/Pneumonia_sep_col_combined'\n",
    "csv_files = [filename for filename in os.listdir(folder_path) if filename.endswith('.csv')]\n",
    "num_csv_files = len(csv_files)\n",
    "print(f\"There are {num_csv_files} CSV files in Pneumonia_sep_col_combined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate each patient file by hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input and output directories\n",
    "input_dir = r'PATH/data/Pneumonia_sep_col_combined'\n",
    "output_dir = r'PATH/data/Pneumonia_agg_col_combined'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Loop through all the csv files in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Read the csv file into a pandas dataframe\n",
    "        df = pd.read_csv(os.path.join(input_dir, filename))\n",
    "\n",
    "        # Convert CHARTTIME to a datetime object\n",
    "        df['CHARTTIME'] = pd.to_datetime(df['CHARTTIME'])\n",
    "\n",
    "        # Group the dataframe by hour and calculate the mean and standard deviation for each group\n",
    "        grouped_df = df.groupby(pd.Grouper(key='CHARTTIME', freq='H')).agg({'HR': [np.mean],\n",
    "                                                                            'spo2': [np.mean],\n",
    "                                                                            'dias': [np.mean],\n",
    "                                                                            'sys': [np.mean],\n",
    "                                                                            'resp': [np.mean]})\n",
    "\n",
    "        # Flatten the multi-level column index to make it easier to work with\n",
    "        grouped_df.columns = ['_'.join(col).strip() for col in grouped_df.columns.values]\n",
    "        \n",
    "        # Add a new column called DATE_HOUR that contains the date and hour for each group\n",
    "        grouped_df['DATE_HOUR'] = grouped_df.index.strftime('%Y-%m-%d %H:00:00')\n",
    "        \n",
    "        # Make DATE_HOUR the first column\n",
    "        cols = ['DATE_HOUR'] + [col for col in grouped_df.columns if col != 'DATE_HOUR']\n",
    "        grouped_df = grouped_df[cols]\n",
    "\n",
    "        # Save the aggregated data to a new csv file in the output directory with the same name\n",
    "        grouped_df.to_csv(os.path.join(output_dir, filename), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 547 CSV files in Pneumonia_agg_col_combined.\n"
     ]
    }
   ],
   "source": [
    "folder_path = r'PATH/data/Pneumonia_agg_col_combined'\n",
    "csv_files = [filename for filename in os.listdir(folder_path) if filename.endswith('.csv')]\n",
    "num_csv_files = len(csv_files)\n",
    "print(f\"There are {num_csv_files} CSV files in Pneumonia_agg_col_combined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select First 4 hours of vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files less than 8 hours of data : 0\n"
     ]
    }
   ],
   "source": [
    "output_dir = r'PATH/data/Pneumonia_agg_col_combined/'\n",
    "\n",
    "count = 0\n",
    "for filename in os.listdir(output_dir):\n",
    "    with open(os.path.join(output_dir, filename), 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        lines = [next(reader) for _ in range(5)]\n",
    "        csvfile.seek(0)  # reset the file pointer\n",
    "        all_lines = [row for row in reader]  # get all lines in the file\n",
    "\n",
    "    if len(all_lines) < 5:\n",
    "        count += 1\n",
    "\n",
    "print('Number of files less than 4 hours of data :', count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate intubated and nonintubated patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_dir = 'PATH/data/INTUBATED/'\n",
    "not_int_dir = 'PATH/data/NOT_INTUBATED/'\n",
    "for directory in [int_dir, not_int_dir]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "directory = r'PATH/data/Pneumonia_agg_col_combined/'\n",
    "flag_file = r'PATH/data/pneumonia_patients.csv'\n",
    "\n",
    "with open(flag_file, 'r') as flagfile:\n",
    "    flag_reader = csv.DictReader(flagfile)\n",
    "    for row in flag_reader:\n",
    "        filename = row['HADM_ID'] + '.csv'\n",
    "        int_flag = row['INTUBATED']\n",
    "        save_directory = int_dir if int_flag else not_int_dir\n",
    "        \n",
    "        if os.path.exists(os.path.join(directory, filename)):\n",
    "            with open(os.path.join(directory, filename), 'r') as csvfile:\n",
    "                reader = csv.reader(csvfile)\n",
    "                lines = [next(reader) for _ in range(5)]\n",
    "\n",
    "            with open(os.path.join(save_directory, filename), 'w') as newfile:\n",
    "                writer = csv.writer(newfile)\n",
    "                writer.writerows(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 154 CSV files in INTUBATED.\n"
     ]
    }
   ],
   "source": [
    "folder_path = r'PATH/data/INTUBATED/'\n",
    "csv_files = [filename for filename in os.listdir(folder_path) if filename.endswith('.csv')]\n",
    "num_csv_files = len(csv_files)\n",
    "print(f\"There are {num_csv_files} CSV files in INTUBATED.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 393 CSV files in NOT_INTUBATED.\n"
     ]
    }
   ],
   "source": [
    "folder_path = r'PATH/data/NOT_INTUBATED/'\n",
    "csv_files = [filename for filename in os.listdir(folder_path) if filename.endswith('.csv')]\n",
    "num_csv_files = len(csv_files)\n",
    "print(f\"There are {num_csv_files} CSV files in NOT_INTUBATED.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten Positive & Negative readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = r'PATH/data/Flattened_Vitals/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "int_dir = r'PATH/data/INTUBATED/'\n",
    "not_int_dir = r'PATH/data/NOT_INTUBATED/'\n",
    "\n",
    "int_out_file = \"PATH/data/Flattened_Vitals/intubated.csv\"\n",
    "nint_out_file = \"PATH/data/Flattened_Vitals/notintubated.csv\"\n",
    "\n",
    "comb_file = \"PATH/data/Flattened_Vitals/combined.csv\"\n",
    "\n",
    "#Delete the current file if it's there.\n",
    "if os.path.exists(nint_out_file):\n",
    "    os.remove(nint_out_file)\n",
    "if os.path.exists(int_out_file):\n",
    "    os.remove(int_out_file)\n",
    "    \n",
    "# Flatten intubated patients\n",
    "for file in os.listdir(int_dir):\n",
    "    with open(os.path.join(int_dir, file)) as samp:\n",
    "        reader = csv.reader(samp)\n",
    "        heading = next(samp) #Skip the header\n",
    "        f = []\n",
    "        f = [os.path.splitext(file)[0]] #add the hadm_id as the first column\n",
    "        for row in reader:\n",
    "            f = f + row[1:] # skip the first column\n",
    "        with open(int_out_file, \"a\") as fp:\n",
    "            wr = csv.writer(fp, dialect='excel')\n",
    "            wr.writerow(f)\n",
    "            \n",
    "            \n",
    "# Flatten non intubated patients\n",
    "for file in os.listdir(not_int_dir):\n",
    "    with open(os.path.join(not_int_dir, file)) as samp:\n",
    "        reader = csv.reader(samp)\n",
    "        heading = next(samp) #Skip the header\n",
    "        f = []\n",
    "        f = [os.path.splitext(file)[0]] #add the hadm_id as the first column\n",
    "        for row in reader:\n",
    "            f = f + row[1:] # skip the first column\n",
    "        with open(nint_out_file, \"a\") as fp:\n",
    "            wr = csv.writer(fp, dialect='excel')\n",
    "            wr.writerow(f)\n",
    "            \n",
    "# Make a combined file with both intubated and non-intubated patients\n",
    "df1 = pd.read_csv(int_out_file, header = None)\n",
    "df2 = pd.read_csv(nint_out_file, header = None)\n",
    "\n",
    "df1.insert(0, '', 1)\n",
    "df2.insert(0, '', 0)\n",
    "\n",
    "# concatenate the two DataFrames\n",
    "result = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# write the concatenated DataFrame to a new CSV file\n",
    "result.to_csv(comb_file, index=False, header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Error/missing values, split test/train, scale for deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "\n",
    "df = pd.read_csv(\"PATH/data/Flattened_Vitals/combined.csv\", header = None)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Separate out results from training/testing data, also eliminate the hadm_id column (location 1)\n",
    "train_results = train_df.iloc[:, 0]\n",
    "train_data = train_df.iloc[:, 2:]\n",
    "test_results = test_df.iloc[:, 0]\n",
    "test_data = test_df.iloc[:, 2:]\n",
    "\n",
    "# Save the split data to separate CSV files\n",
    "train_results.to_csv(\"PATH/data/Flattened_Vitals/train_results.csv\", index=False)\n",
    "test_results.to_csv(\"PATH/data/Flattened_Vitals/test_results.csv\", index=False)\n",
    "train_data.to_csv(\"PATH/data/Flattened_Vitals/train_data.csv\", index=False)\n",
    "test_data.to_csv(\"PATH/Flattened_Vitals/test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all error values, non-numbers, and zeros with blanks - do this BEFORE creating a scaled version\n",
    "# output these files back to the same location\n",
    "\n",
    "input_dir = r'PATH/data/Flattened_Vitals/'\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        with open(os.path.join(input_dir, filename), 'r') as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            new_rows = []\n",
    "            for row in reader:\n",
    "                new_row = []\n",
    "                for cell in row:\n",
    "                    #Replace negatives and '0' with empty string\n",
    "                    if cell.startswith('-') or cell == '0':\n",
    "                        cell = ''\n",
    "                    #Replace non-numbers, blanks with '0'\n",
    "                    try:\n",
    "                        float(cell)\n",
    "                    except ValueError:\n",
    "                        cell = ''\n",
    "                    new_row.append(cell)\n",
    "                new_rows.append(new_row)\n",
    "        with open(os.path.join(input_dir, filename), 'w', newline='') as outfile:\n",
    "            writer = csv.writer(outfile)\n",
    "            writer.writerows(new_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scaled version, before replacing missing values with 0's\n",
    "test_dat = pd.read_csv(r'PATH/data/Flattened_Vitals/test_data.csv', header = None)\n",
    "train_dat = pd.read_csv(r'PATH/data/Flattened_Vitals/train_data.csv', header = None)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(train_dat)\n",
    "X_test = sc.transform (test_dat)\n",
    "\n",
    "np.savetxt(\"PATH/data/Flattened_Vitals/scaled_test_data.csv\", X_test, delimiter=\",\")\n",
    "np.savetxt(\"PATH/data/Flattened_Vitals/scaled_train_data.csv\", X_train, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace all empties with 0's, and write out both the scaled, and non-scaled to the Cleaned_Vitals folder\n",
    "\n",
    "input_dir = r'PATH/data/Flattened_Vitals/'\n",
    "output_dir = r'PATH/data/Cleaned_Vitals/'\n",
    "\n",
    "# create the output directory if it doesn't already exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        with open(os.path.join(input_dir, filename), 'r') as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            new_rows = []\n",
    "            for row in reader:\n",
    "                new_row = []\n",
    "                for cell in row:\n",
    "                    # Replace 'nan' with '0'\n",
    "                    if cell == 'nan':\n",
    "                        cell = '0'\n",
    "                    #Replace non-numbers, blanks with '0'\n",
    "                    try:\n",
    "                        float(cell)\n",
    "                    except ValueError:\n",
    "                        cell = '0'\n",
    "                    new_row.append(cell)\n",
    "                new_rows.append(new_row)\n",
    "        with open(os.path.join(output_dir, filename), 'w', newline='') as outfile:\n",
    "            writer = csv.writer(outfile)\n",
    "            writer.writerows(new_rows)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
