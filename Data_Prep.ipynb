{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, copy\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Read in data... just do this once.\n",
    "path = r'PATH/data/pneumona_reduced_vitals.csv'\n",
    "vitals = pd.read_csv(path)\n",
    "vitals = np.array(vitals)\n",
    "\n",
    "# Structure of vitals rows\n",
    "#ROW_ID | SUBJECT_ID | HADM_ID | ICUSTAY_ID | ITEMID | CHARTTIME | STORETIME | CGID | VALUE | VALUENUM | VALUEUOM | WARNING | ERROR | RESULTSTATUS | STOPPED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique PATIENT_ID values and count them\n",
    "unique_ids = np.unique(vitals[:, 1])\n",
    "num_unique_ids = len(unique_ids)\n",
    "unique_hids = np.unique(vitals[:, 2])\n",
    "num_unique_hids = len(unique_hids)\n",
    "print(\"Number of distinct PATIENT_IDs:\", num_unique_ids)\n",
    "print(\"Number of distinct HADM_IDs:\", num_unique_hids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Pneumonia patients & write out vitals to distinct .csv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output folder if it doesn't exist\n",
    "output_folder = 'Pneumonia_sep'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Open the mixed CSV file for reading\n",
    "with open(path, 'r') as mixed_file:\n",
    "    reader = csv.DictReader(mixed_file)\n",
    "    \n",
    "    # Create a dictionary to store the output file handles\n",
    "    output_files = {}\n",
    "    \n",
    "    # Iterate over the rows in the mixed file\n",
    "    for row in reader:\n",
    "        hadm_id = row['HADM_ID']\n",
    "        output_file_name = f'{output_folder}/{hadm_id}.csv'\n",
    "        \n",
    "        # If this is the first row with this HADM ID, open a new output file\n",
    "        if hadm_id not in output_files:\n",
    "            output_files[hadm_id] = open(output_file_name, 'w', newline='')\n",
    "            writer = csv.DictWriter(output_files[hadm_id], fieldnames=reader.fieldnames)\n",
    "            writer.writeheader()\n",
    "        \n",
    "        # Write the row to the output file\n",
    "        writer = csv.DictWriter(output_files[hadm_id], fieldnames=reader.fieldnames)\n",
    "        writer.writerow(row)\n",
    "    \n",
    "    # Close all the output files\n",
    "    for output_file in output_files.values():\n",
    "        output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat each patient-visit level file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Set up the file paths for the input and output directories\n",
    "input_dir = r'PATH/data/Pneumonia_sep'\n",
    "output_dir = r'PATH/data/Pneumonia_sep_processed'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# create a list of the ITEMID values you want to extract\n",
    "item_ids = [211, 220045, #Heart Rate\n",
    "            646, 220227, #Sp02 (blood oxygen level)\n",
    "            8368, 220051, #Arterial BP Diastolic\n",
    "            51, 220050, #Arterial BP Systolic\n",
    "            615, 224690] #Total Resp\n",
    "\n",
    "csv_files = [filename for filename in os.listdir(input_dir) if filename.endswith('.csv')]\n",
    "\n",
    "# loop over each file in the input directory\n",
    "for filename in csv_files:\n",
    "    # read the CSV file into a pandas dataframe\n",
    "    df = pd.read_csv(os.path.join(input_dir, filename))\n",
    "    \n",
    "    # group the dataframe by CHARTTIME\n",
    "    grouped = df.groupby(['CHARTTIME'])\n",
    "    result = pd.DataFrame()\n",
    "\n",
    "    # iterate over each group and extract the ITEMID values\n",
    "    for name, group in grouped:\n",
    "        # create a new row for the result dataframe\n",
    "        row = {'CHARTTIME': name}\n",
    "        for item_id in item_ids:\n",
    "            # extract the rows with the specified ITEMID at CHARTTIME\n",
    "            filtered = group[(group['ITEMID'] == item_id) & (group['CHARTTIME'] == name)]\n",
    "            if not filtered.empty:\n",
    "                # if there is a row with the ITEMID at CHARTTIME, extract the value\n",
    "                row[f'ITEMID_{item_id}'] = filtered.iloc[0]['VALUE']\n",
    "            else:\n",
    "                # if there is no row with the ITEMID at CHARTTIME, leave the spot blank\n",
    "                row[f'ITEMID_{item_id}'] = ''\n",
    "        # add the row to the result dataframe\n",
    "        result = result.append(row, ignore_index=True)\n",
    "\n",
    "    # save the result dataframe to a new CSV file in the output directory\n",
    "    output_filename = os.path.join(output_dir, filename)\n",
    "    result.to_csv(output_filename, index=False)\n",
    "    print(f\"Processed {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r'PATH/data/Pneumonia_sep_processed'\n",
    "csv_files = [filename for filename in os.listdir(folder_path) if filename.endswith('.csv')]\n",
    "num_csv_files = len(csv_files)\n",
    "print(f\"There are {num_csv_files} CSV files in Pneumonia_sep_processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the columns of equivalent readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the file paths for the input and output directories\n",
    "input_dir = r'PATH/data/Pneumonia_sep_processed'\n",
    "output_dir = r'PATH/data/Pneumonia_sep_col_combined'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "# loop through each csv file in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        # read in the csv file\n",
    "        df = pd.read_csv(os.path.join(input_dir, filename))\n",
    "        \n",
    "        # combine the specified columns and rename them\n",
    "        df['HR'] = df[['ITEMID_211', 'ITEMID_220045']].fillna(method='backfill', axis=1)['ITEMID_211']\n",
    "        df['spo2'] = df[['ITEMID_646', 'ITEMID_220227']].fillna(method='backfill', axis=1)['ITEMID_646']\n",
    "        df['dias'] = df[['ITEMID_8368', 'ITEMID_220051']].fillna(method='backfill', axis=1)['ITEMID_8368']\n",
    "        df['sys'] = df[['ITEMID_51', 'ITEMID_220050']].fillna(method='backfill', axis=1)['ITEMID_51']\n",
    "        df['resp'] = df[['ITEMID_615', 'ITEMID_224690']].fillna(method='backfill', axis=1)['ITEMID_615']\n",
    "        \n",
    "        # drop the original columns & reorder what's left\n",
    "        df = df.drop(['ITEMID_211', 'ITEMID_220045', 'ITEMID_646', 'ITEMID_220227', \n",
    "                      'ITEMID_8368', 'ITEMID_220051', 'ITEMID_51', 'ITEMID_220050', 'ITEMID_615', 'ITEMID_224690'], axis=1)\n",
    "        df = df[['CHARTTIME', 'HR', 'sys', 'dias', #'mean',\n",
    "                 'resp','spo2']] \n",
    "\n",
    "        # write the modified dataframe to a new csv file in the output directory\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Processed {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r'PATH/data/Pneumonia_sep_col_combined'\n",
    "csv_files = [filename for filename in os.listdir(folder_path) if filename.endswith('.csv')]\n",
    "num_csv_files = len(csv_files)\n",
    "print(f\"There are {num_csv_files} CSV files in Pneumonia_sep_col_combined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate each patient file by hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input and output directories\n",
    "input_dir = r'PATH/data/Pneumonia_sep_col_combined'\n",
    "output_dir = r'PATH/data/Pneumonia_agg_col_combined'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Loop through all the csv files in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Read the csv file into a pandas dataframe\n",
    "        df = pd.read_csv(os.path.join(input_dir, filename))\n",
    "\n",
    "        # Convert CHARTTIME to a datetime object\n",
    "        df['CHARTTIME'] = pd.to_datetime(df['CHARTTIME'])\n",
    "\n",
    "        # Group the dataframe by hour and calculate the mean and standard deviation for each group\n",
    "        grouped_df = df.groupby(pd.Grouper(key='CHARTTIME', freq='H')).agg({'HR': [np.mean],\n",
    "                                                                            'spo2': [np.mean],\n",
    "                                                                            'dias': [np.mean],\n",
    "                                                                            'sys': [np.mean],\n",
    "                                                                            'resp': [np.mean]})\n",
    "\n",
    "        # Flatten the multi-level column index to make it easier to work with\n",
    "        grouped_df.columns = ['_'.join(col).strip() for col in grouped_df.columns.values]\n",
    "        \n",
    "        # Add a new column called DATE_HOUR that contains the date and hour for each group\n",
    "        grouped_df['DATE_HOUR'] = grouped_df.index.strftime('%Y-%m-%d %H:00:00')\n",
    "        \n",
    "        # Make DATE_HOUR the first column\n",
    "        cols = ['DATE_HOUR'] + [col for col in grouped_df.columns if col != 'DATE_HOUR']\n",
    "        grouped_df = grouped_df[cols]\n",
    "\n",
    "        # Save the aggregated data to a new csv file in the output directory with the same name\n",
    "        grouped_df.to_csv(os.path.join(output_dir, filename), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r'PATH/data/Pneumonia_agg_col_combined'\n",
    "csv_files = [filename for filename in os.listdir(folder_path) if filename.endswith('.csv')]\n",
    "num_csv_files = len(csv_files)\n",
    "print(f\"There are {num_csv_files} CSV files in Pneumonia_agg_col_combined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select First 4 hours of vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = r'PATH/data/Pneumonia_agg_col_combined/'\n",
    "\n",
    "count = 0\n",
    "for filename in os.listdir(output_dir):\n",
    "    with open(os.path.join(output_dir, filename), 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        lines = [next(reader) for _ in range(5)]\n",
    "        csvfile.seek(0)  # reset the file pointer\n",
    "        all_lines = [row for row in reader]  # get all lines in the file\n",
    "\n",
    "    if len(all_lines) < 5:\n",
    "        count += 1\n",
    "\n",
    "print('Number of files less than 4 hours of data :', count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate intubated and nonintubated patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_dir = 'PATH/data/INTUBATED/'\n",
    "not_int_dir = 'PATH/data/NOT_INTUBATED/'\n",
    "for directory in [int_dir, not_int_dir]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "directory = r'PATH/data/Pneumonia_agg_col_combined/'\n",
    "flag_file = r'PATH/data/pneumonia_patients.csv'\n",
    "\n",
    "with open(flag_file, 'r') as flagfile:\n",
    "    flag_reader = csv.DictReader(flagfile)\n",
    "    for row in flag_reader:\n",
    "        filename = row['HADM_ID'] + '.csv'\n",
    "        int_flag = row['INTUBATED']\n",
    "        save_directory = int_dir if int_flag else not_int_dir\n",
    "        \n",
    "        if os.path.exists(os.path.join(directory, filename)):\n",
    "            with open(os.path.join(directory, filename), 'r') as csvfile:\n",
    "                reader = csv.reader(csvfile)\n",
    "                lines = [next(reader) for _ in range(5)]\n",
    "\n",
    "            with open(os.path.join(save_directory, filename), 'w') as newfile:\n",
    "                writer = csv.writer(newfile)\n",
    "                writer.writerows(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r'PATH/data/INTUBATED/'\n",
    "csv_files = [filename for filename in os.listdir(folder_path) if filename.endswith('.csv')]\n",
    "num_csv_files = len(csv_files)\n",
    "print(f\"There are {num_csv_files} CSV files in INTUBATED.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r'PATH/data/NOT_INTUBATED/'\n",
    "csv_files = [filename for filename in os.listdir(folder_path) if filename.endswith('.csv')]\n",
    "num_csv_files = len(csv_files)\n",
    "print(f\"There are {num_csv_files} CSV files in NOT_INTUBATED.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten Positive & Negative readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = r'PATH/data/Flattened_Vitals/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "int_dir = r'PATH/data/INTUBATED/'\n",
    "not_int_dir = r'PATH/data/NOT_INTUBATED/'\n",
    "\n",
    "int_out_file = \"PATH/data/Flattened_Vitals/intubated.csv\"\n",
    "nint_out_file = \"PATH/data/Flattened_Vitals/notintubated.csv\"\n",
    "\n",
    "comb_file = \"PATH/data/Flattened_Vitals/combined.csv\"\n",
    "\n",
    "#Delete the current file if it's there.\n",
    "if os.path.exists(nint_out_file):\n",
    "    os.remove(nint_out_file)\n",
    "if os.path.exists(int_out_file):\n",
    "    os.remove(int_out_file)\n",
    "    \n",
    "# Flatten intubated patients\n",
    "for file in os.listdir(int_dir):\n",
    "    with open(os.path.join(int_dir, file)) as samp:\n",
    "        reader = csv.reader(samp)\n",
    "        heading = next(samp) #Skip the header\n",
    "        f = []\n",
    "        f = [os.path.splitext(file)[0]] #add the hadm_id as the first column\n",
    "        for row in reader:\n",
    "            f = f + row[1:] # skip the first column\n",
    "        with open(int_out_file, \"a\") as fp:\n",
    "            wr = csv.writer(fp, dialect='excel')\n",
    "            wr.writerow(f)\n",
    "            \n",
    "            \n",
    "# Flatten non intubated patients\n",
    "for file in os.listdir(not_int_dir):\n",
    "    with open(os.path.join(not_int_dir, file)) as samp:\n",
    "        reader = csv.reader(samp)\n",
    "        heading = next(samp) #Skip the header\n",
    "        f = []\n",
    "        f = [os.path.splitext(file)[0]] #add the hadm_id as the first column\n",
    "        for row in reader:\n",
    "            f = f + row[1:] # skip the first column\n",
    "        with open(nint_out_file, \"a\") as fp:\n",
    "            wr = csv.writer(fp, dialect='excel')\n",
    "            wr.writerow(f)\n",
    "            \n",
    "# Make a combined file with both intubated and non-intubated patients\n",
    "df1 = pd.read_csv(int_out_file, header = None)\n",
    "df2 = pd.read_csv(nint_out_file, header = None)\n",
    "\n",
    "df1.insert(0, '', 1)\n",
    "df2.insert(0, '', 0)\n",
    "\n",
    "# concatenate the two DataFrames\n",
    "result = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# write the concatenated DataFrame to a new CSV file\n",
    "result.to_csv(comb_file, index=False, header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Error/missing values, split test/train, scale for deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "\n",
    "df = pd.read_csv(\"PATH/data/Flattened_Vitals/combined.csv\", header = None)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Separate out results from training/testing data, also eliminate the hadm_id column (location 1)\n",
    "train_results = train_df.iloc[:, 0]\n",
    "train_data = train_df.iloc[:, 2:]\n",
    "test_results = test_df.iloc[:, 0]\n",
    "test_data = test_df.iloc[:, 2:]\n",
    "\n",
    "# Save the split data to separate CSV files\n",
    "train_results.to_csv(\"PATH/data/Flattened_Vitals/train_results.csv\", index=False)\n",
    "test_results.to_csv(\"PATH/data/Flattened_Vitals/test_results.csv\", index=False)\n",
    "train_data.to_csv(\"PATH/data/Flattened_Vitals/train_data.csv\", index=False)\n",
    "test_data.to_csv(\"PATH/Flattened_Vitals/test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all error values, non-numbers, and zeros with blanks - do this BEFORE creating a scaled version\n",
    "# output these files back to the same location\n",
    "\n",
    "input_dir = r'PATH/data/Flattened_Vitals/'\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        with open(os.path.join(input_dir, filename), 'r') as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            new_rows = []\n",
    "            for row in reader:\n",
    "                new_row = []\n",
    "                for cell in row:\n",
    "                    #Replace negatives and '0' with empty string\n",
    "                    if cell.startswith('-') or cell == '0':\n",
    "                        cell = ''\n",
    "                    #Replace non-numbers, blanks with '0'\n",
    "                    try:\n",
    "                        float(cell)\n",
    "                    except ValueError:\n",
    "                        cell = ''\n",
    "                    new_row.append(cell)\n",
    "                new_rows.append(new_row)\n",
    "        with open(os.path.join(input_dir, filename), 'w', newline='') as outfile:\n",
    "            writer = csv.writer(outfile)\n",
    "            writer.writerows(new_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scaled version, before replacing missing values with 0's\n",
    "test_dat = pd.read_csv(r'PATH/data/Flattened_Vitals/test_data.csv', header = None)\n",
    "train_dat = pd.read_csv(r'PATH/data/Flattened_Vitals/train_data.csv', header = None)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(train_dat)\n",
    "X_test = sc.transform (test_dat)\n",
    "\n",
    "np.savetxt(\"PATH/data/Flattened_Vitals/scaled_test_data.csv\", X_test, delimiter=\",\")\n",
    "np.savetxt(\"PATH/data/Flattened_Vitals/scaled_train_data.csv\", X_train, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace all empties with 0's, and write out both the scaled, and non-scaled to the Cleaned_Vitals folder\n",
    "\n",
    "input_dir = r'PATH/data/Flattened_Vitals/'\n",
    "output_dir = r'PATH/data/Cleaned_Vitals/'\n",
    "\n",
    "# create the output directory if it doesn't already exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        with open(os.path.join(input_dir, filename), 'r') as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            new_rows = []\n",
    "            for row in reader:\n",
    "                new_row = []\n",
    "                for cell in row:\n",
    "                    # Replace 'nan' with '0'\n",
    "                    if cell == 'nan':\n",
    "                        cell = '0'\n",
    "                    #Replace non-numbers, blanks with '0'\n",
    "                    try:\n",
    "                        float(cell)\n",
    "                    except ValueError:\n",
    "                        cell = '0'\n",
    "                    new_row.append(cell)\n",
    "                new_rows.append(new_row)\n",
    "        with open(os.path.join(output_dir, filename), 'w', newline='') as outfile:\n",
    "            writer = csv.writer(outfile)\n",
    "            writer.writerows(new_rows)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
